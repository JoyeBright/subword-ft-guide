# Analysis of Vocabulary and Subword Tokenization Settings for Optimal Fine-tuning of MT

**Accepted at RANLP 2025**

This repository contains the code and data for our RANLP 2025 paper, which systematically investigates how different vocabulary and subword (BPE) tokenization strategies impact the fine-tuning of neural machine translation (NMT) models for domain adaptation. Using English-German general and medical domain datasets, we show that optimal fine-tuning is achieved by deriving both vocabulary and BPE from in-domain data, while ensuring good coverage of the original modelâ€™s vocabulary. The results provide practical guidelines for adapting NMT systems to new domains.
